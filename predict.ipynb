{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NaN values before imputation:\n",
      "Age                      18705\n",
      "Gender                       0\n",
      "Annual Income            44949\n",
      "Marital Status           18529\n",
      "Number of Dependents    109672\n",
      "Education Level              0\n",
      "Occupation              358075\n",
      "Health Score             74076\n",
      "Location                     0\n",
      "Policy Type                  0\n",
      "Previous Claims         364029\n",
      "Vehicle Age                  6\n",
      "Credit Score            137882\n",
      "Insurance Duration           1\n",
      "Customer Feedback        77824\n",
      "Smoking Status               0\n",
      "Exercise Frequency           0\n",
      "Property Type                0\n",
      "Premium Amount               0\n",
      "dtype: int64\n",
      "\n",
      "NaN values after imputation:\n",
      "Age                     0\n",
      "Gender                  0\n",
      "Annual Income           0\n",
      "Marital Status          0\n",
      "Number of Dependents    0\n",
      "Education Level         0\n",
      "Occupation              0\n",
      "Health Score            0\n",
      "Location                0\n",
      "Policy Type             0\n",
      "Previous Claims         0\n",
      "Vehicle Age             0\n",
      "Credit Score            0\n",
      "Insurance Duration      0\n",
      "Customer Feedback       0\n",
      "Smoking Status          0\n",
      "Exercise Frequency      0\n",
      "Property Type           0\n",
      "Premium Amount          0\n",
      "dtype: int64\n",
      "\n",
      "Segment Distribution:\n",
      "Low_Risk_Premium: 245,315 (20.4%)\n",
      "High_Risk_Premium: 110,833 (9.2%)\n",
      "Young_Urban_Professional: 98,418 (8.2%)\n",
      "Family_Suburban: 219,595 (18.3%)\n",
      "Senior_Stable: 161,327 (13.4%)\n",
      "Budget_Basic: 159,538 (13.3%)\n",
      "Premium_Healthy: 73,108 (6.1%)\n",
      "High_Value_Property: 47,679 (4.0%)\n",
      "Default_Segment: 429,859 (35.8%)\n",
      "\n",
      "Total records: 1,200,000\n",
      "Total assigned: 1,545,672\n",
      "Records per segment on average: 171,741.3\n",
      "\n",
      "Processing Low_Risk_Premium segment...\n",
      "\n",
      "Segment Length 245315...\n",
      "Train R2: 0.0814\n",
      "Test R2: 0.0517\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 647.42\n",
      "Median Absolute Error: 502.36\n",
      "Mean % Error: 312.70%\n",
      "RMSE: 852.73\n",
      "CV Mean R2: 0.0511 (+/- 0.0042)\n",
      "\n",
      "Processing High_Risk_Premium segment...\n",
      "\n",
      "Segment Length 110833...\n",
      "Train R2: 0.0490\n",
      "Test R2: 0.0127\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 678.22\n",
      "Median Absolute Error: 548.98\n",
      "Mean % Error: 318.16%\n",
      "RMSE: 873.88\n",
      "CV Mean R2: 0.0129 (+/- 0.0037)\n",
      "\n",
      "Processing Young_Urban_Professional segment...\n",
      "\n",
      "Segment Length 98418...\n",
      "Train R2: 0.1125\n",
      "Test R2: 0.0488\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 657.44\n",
      "Median Absolute Error: 513.99\n",
      "Mean % Error: 306.16%\n",
      "RMSE: 869.59\n",
      "CV Mean R2: 0.0471 (+/- 0.0038)\n",
      "\n",
      "Processing Family_Suburban segment...\n",
      "\n",
      "Segment Length 219595...\n",
      "Train R2: 0.0753\n",
      "Test R2: 0.0366\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 646.21\n",
      "Median Absolute Error: 507.19\n",
      "Mean % Error: 301.95%\n",
      "RMSE: 851.49\n",
      "CV Mean R2: 0.0394 (+/- 0.0069)\n",
      "\n",
      "Processing Senior_Stable segment...\n",
      "\n",
      "Segment Length 161327...\n",
      "Train R2: 0.0994\n",
      "Test R2: 0.0461\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 643.92\n",
      "Median Absolute Error: 496.53\n",
      "Mean % Error: 310.01%\n",
      "RMSE: 853.07\n",
      "CV Mean R2: 0.0521 (+/- 0.0034)\n",
      "\n",
      "Processing Budget_Basic segment...\n",
      "\n",
      "Segment Length 159538...\n",
      "Train R2: 0.0612\n",
      "Test R2: 0.0213\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 633.49\n",
      "Median Absolute Error: 499.73\n",
      "Mean % Error: 303.15%\n",
      "RMSE: 826.84\n",
      "CV Mean R2: 0.0209 (+/- 0.0044)\n",
      "\n",
      "Processing Premium_Healthy segment...\n",
      "\n",
      "Segment Length 73108...\n",
      "Train R2: 0.1178\n",
      "Test R2: 0.0401\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 653.70\n",
      "Median Absolute Error: 518.27\n",
      "Mean % Error: 296.86%\n",
      "RMSE: 863.80\n",
      "CV Mean R2: 0.0443 (+/- 0.0066)\n",
      "\n",
      "Processing High_Value_Property segment...\n",
      "\n",
      "Segment Length 47679...\n",
      "Train R2: 0.0984\n",
      "Test R2: 0.0733\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 685.05\n",
      "Median Absolute Error: 517.18\n",
      "Mean % Error: 338.52%\n",
      "RMSE: 909.98\n",
      "CV Mean R2: 0.0748 (+/- 0.0143)\n",
      "\n",
      "Processing Default_Segment segment...\n",
      "\n",
      "Segment Length 429859...\n",
      "Train R2: 0.0634\n",
      "Test R2: 0.0355\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 635.81\n",
      "Median Absolute Error: 501.79\n",
      "Mean % Error: 292.06%\n",
      "RMSE: 837.92\n",
      "CV Mean R2: 0.0381 (+/- 0.0031)\n",
      "\n",
      "Segment Performance Summary:\n",
      "                          segment_size  train_r2   test_r2  cv_mean_r2  \\\n",
      "High_Value_Property             429859  0.098406  0.073266    0.074772   \n",
      "Low_Risk_Premium                429859  0.081353  0.051711    0.051095   \n",
      "Young_Urban_Professional        429859  0.112465  0.048773    0.047128   \n",
      "Senior_Stable                   429859  0.099379  0.046104    0.052131   \n",
      "Premium_Healthy                 429859  0.117791  0.040050    0.044278   \n",
      "Family_Suburban                 429859  0.075302  0.036606    0.039423   \n",
      "Default_Segment                 429859  0.063362  0.035530    0.038063   \n",
      "Budget_Basic                    429859  0.061212  0.021309    0.020914   \n",
      "High_Risk_Premium               429859  0.048988  0.012671    0.012879   \n",
      "\n",
      "                          cv_std_r2         mae        mape  \n",
      "High_Value_Property        0.007162  685.053899  338.517205  \n",
      "Low_Risk_Premium           0.002086  647.418835  312.696776  \n",
      "Young_Urban_Professional   0.001924  657.443549  306.156184  \n",
      "Senior_Stable              0.001691  643.916507  310.008704  \n",
      "Premium_Healthy            0.003298  653.697246  296.859046  \n",
      "Family_Suburban            0.003473  646.211542  301.954785  \n",
      "Default_Segment            0.001564  635.805445  292.057387  \n",
      "Budget_Basic               0.002213  633.487463  303.146883  \n",
      "High_Risk_Premium          0.001838  678.219548  318.155273  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.impute import KNNImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def preprocess_insurance_data(df):\n",
    "    df = df.copy()\n",
    "    ids = df['id'].copy()\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df.drop(['id', 'Policy Start Date'], axis=1, inplace=True)\n",
    "    df.replace([float('inf'), float('-inf')], np.nan, inplace=True)\n",
    "    \n",
    "    print(\"\\nNaN values before imputation:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # First handle categorical variables using complete columns\n",
    "    # Group by complete columns that might have logical relationships\n",
    "    \n",
    "    # Occupation (358,075 missing)\n",
    "    # Education Level and Location are complete and likely related to occupation\n",
    "    df['Occupation'] = df.groupby(['Education Level', 'Location'], observed=True)['Occupation'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "    )\n",
    "    \n",
    "    # Health Score (74,076 missing) could relate to Exercise Frequency and Smoking Status\n",
    "    df['Health Score'] = df.groupby(['Exercise Frequency', 'Smoking Status'], observed=True)['Health Score'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    \n",
    "    # Credit Score (137,882 missing) might relate to Property Type and Policy Type\n",
    "    df['Credit Score'] = df.groupby(['Property Type', 'Policy Type'], observed=True)['Credit Score'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    \n",
    "    # Marital Status (18,529 missing) might relate to Property Type\n",
    "    df['Marital Status'] = df.groupby('Property Type', observed=True)['Marital Status'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "    )\n",
    "    \n",
    "    # For remaining numeric columns, use grouping by Policy Type and Property Type\n",
    "    numeric_cols = ['Age', 'Annual Income', 'Number of Dependents', 'Previous Claims', \n",
    "                   'Vehicle Age', 'Insurance Duration']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df[col] = df.groupby(['Policy Type', 'Property Type'], observed=True)[col].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "    \n",
    "    # Customer Feedback can use Policy Type\n",
    "    df['Customer Feedback'] = df.groupby('Policy Type', observed=True)['Customer Feedback'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nNaN values after imputation:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    return df, ids\n",
    "\n",
    "def segment_data(df):\n",
    "    segments = {\n",
    "        'Low_Risk_Premium': (\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.5)) &\n",
    "            (df['Health Score'] > df['Health Score'].quantile(0.5)) &\n",
    "            (df['Insurance Duration'] > 1)\n",
    "        ),\n",
    "        'High_Risk_Premium': (\n",
    "            (df['Previous Claims'] >= 2) &\n",
    "            ((df['Credit Score'] < df['Credit Score'].quantile(0.3)) |\n",
    "             (df['Health Score'] < df['Health Score'].quantile(0.3)))\n",
    "        ),\n",
    "        'Young_Urban_Professional': (\n",
    "            (df['Age'] <= df['Age'].quantile(0.4)) &\n",
    "            (df['Location'] == 'Urban') &\n",
    "            (df['Annual Income'] > df['Annual Income'].quantile(0.4)) &\n",
    "            (df['Insurance Duration'] > 0)\n",
    "        ),\n",
    "        'Family_Suburban': (\n",
    "            (df['Number of Dependents'] >= 1) &\n",
    "            (df['Location'].isin(['Suburban', 'Rural'])) &\n",
    "            (df['Marital Status'] == 'Married') &\n",
    "            (df['Insurance Duration'] > 0)\n",
    "        ),\n",
    "        'Senior_Stable': (\n",
    "            (df['Age'] >= df['Age'].quantile(0.6)) &\n",
    "            (df['Insurance Duration'] > df['Insurance Duration'].quantile(0.4)) &\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.4))\n",
    "        ),\n",
    "        'Budget_Basic': (\n",
    "            (df['Annual Income'] <= df['Annual Income'].quantile(0.4)) &\n",
    "            ((df['Policy Type'] == 'Basic') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Insurance Duration'] >= 0)\n",
    "        ),\n",
    "        'Premium_Healthy': (\n",
    "            ((df['Policy Type'] == 'Premium') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Exercise Frequency'].isin(['Daily', 'Weekly', 'Monthly'])) &\n",
    "            (df['Health Score'] > df['Health Score'].quantile(0.6)) &\n",
    "            (df['Annual Income'] > df['Annual Income'].quantile(0.4))\n",
    "        ),\n",
    "        'High_Value_Property': (\n",
    "            (df['Property Type'].isin(['House', 'Condo'])) &\n",
    "            (df['Annual Income'] >= df['Annual Income'].quantile(0.6)) &\n",
    "            ((df['Policy Type'] == 'Premium') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.5))\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Create a mask for all data assigned to a segment\n",
    "    assigned_mask = np.zeros(len(df), dtype=bool)\n",
    "    for mask in segments.values():\n",
    "        assigned_mask |= mask\n",
    "\n",
    "    # Create a default segment for unassigned data\n",
    "    segments['Default_Segment'] = ~assigned_mask\n",
    "\n",
    "    # Print distribution for debugging\n",
    "    total_records = len(df)\n",
    "    print(\"\\nSegment Distribution:\")\n",
    "    total_assigned = 0\n",
    "    for name, mask in segments.items():\n",
    "        segment_size = mask.sum()\n",
    "        total_assigned += segment_size\n",
    "        percentage = (segment_size / total_records) * 100\n",
    "        print(f\"{name}: {segment_size:,} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Additional debug info\n",
    "    print(f\"\\nTotal records: {total_records:,}\")\n",
    "    print(f\"Total assigned: {total_assigned:,}\")\n",
    "    print(f\"Records per segment on average: {total_assigned/len(segments):,.1f}\")\n",
    "\n",
    "    return segments\n",
    "\n",
    "class InsuranceSegmentModel:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize the model with a DataFrame\"\"\"\n",
    "        self.df = df\n",
    "        self.processed_df = None\n",
    "        self.segment_results = {}\n",
    "        self.cv_results = {}\n",
    "\n",
    "        # NEW: Create standard category mappings at initialization\n",
    "        self.categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if 'Premium Amount' in self.categorical_features:\n",
    "            self.categorical_features.remove('Premium Amount')\n",
    "            \n",
    "        self.category_mappings = {\n",
    "            col: sorted(df[col].unique()) for col in self.categorical_features\n",
    "        }\n",
    "\n",
    "        self.segments = segment_data(df)\n",
    "\n",
    "        # Define model configurations\n",
    "        self.segment_configs = {\n",
    "            'High_Value_Property': {\n",
    "                'model': StackingRegressor(\n",
    "                    estimators=[\n",
    "                        ('rf', RandomForestRegressor(\n",
    "                            n_estimators=80,  # Reduced complexity\n",
    "                            max_depth=6,       # Lower depth to prevent overfitting\n",
    "                            min_samples_leaf=20,\n",
    "                            random_state=42\n",
    "                        )),\n",
    "                        ('gbm', GradientBoostingRegressor(\n",
    "                            n_estimators=80,  # Reduced complexity\n",
    "                            learning_rate=0.08,  # Slightly lower to improve generalization\n",
    "                            max_depth=4,  # Lower depth to prevent overfitting\n",
    "                            random_state=42\n",
    "                        ))\n",
    "                    ],\n",
    "                    final_estimator=LassoCV(cv=5, random_state=42),  # Increased CV folds\n",
    "                    cv=5,  # Increased CV folds\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "            },\n",
    "            'Budget_Basic': {\n",
    "                'model': RandomForestRegressor(  # Adjusted parameters for possibly better generalization\n",
    "                    n_estimators=200,\n",
    "                    max_depth=10,  # Slightly more depth to capture more complex patterns\n",
    "                    min_samples_leaf=25,  # Allowing more fine-grained leaf nodes\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                ),\n",
    "            },\n",
    "            'High_Risk_Premium': {\n",
    "                'model': GradientBoostingRegressor(  # Adjust parameters to improve fitting\n",
    "                    n_estimators=120,  # More estimators for better learning\n",
    "                    learning_rate=0.08,  # Slightly lower to improve stability\n",
    "                    max_depth=5,  # Increased depth to capture more complex relationships\n",
    "                    random_state=42\n",
    "                ),\n",
    "            },\n",
    "            'default': {\n",
    "                'model': RandomForestRegressor(\n",
    "                    n_estimators=120,  # Slightly more estimators\n",
    "                    max_depth=10,  # Increased depth to potentially improve model capture\n",
    "                    min_samples_leaf=25,  # Smaller leaf size for better granularity\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                ),\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def create_segment_pipeline(self, segment_name):\n",
    "        \"\"\"Creates a pipeline specific to a segment\"\"\"\n",
    "        config = self.segment_configs.get(segment_name, self.segment_configs['default'])\n",
    "        \n",
    "        # Define feature groups\n",
    "        numeric_features = self.df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        if 'Premium Amount' in numeric_features:\n",
    "            numeric_features.remove('Premium Amount')\n",
    "                \n",
    "        # Create transformers\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # CHANGED: Use predefined category mappings\n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(\n",
    "                categories=[self.category_mappings[col] for col in self.categorical_features],\n",
    "                sparse_output=False,\n",
    "                handle_unknown='ignore'\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Create preprocessor\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, self.categorical_features)\n",
    "            ],\n",
    "            sparse_threshold=0\n",
    "        )\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', config['model'])\n",
    "        ])\n",
    "        \n",
    "        return pipeline\n",
    "        \n",
    "    def evaluate_predictions(self, y_true, y_pred):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'r2_score': r2_score(y_true, y_pred),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
    "            'median_ae': np.median(np.abs(y_true - y_pred))\n",
    "        }\n",
    "    \n",
    "    def train_segment_model(self, X_seg, y_seg, segment_name):\n",
    "        \"\"\"Trains and evaluates a model for a specific customer segment\"\"\"\n",
    "        # Create pipeline\n",
    "        pipeline = self.create_segment_pipeline(segment_name)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_seg, y_seg, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = pipeline.predict(X_train)\n",
    "        test_predictions = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_metrics = self.evaluate_predictions(y_train, train_predictions)\n",
    "        test_metrics = self.evaluate_predictions(y_test, test_predictions)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, X_seg, y_seg,\n",
    "            cv=5,\n",
    "            scoring=make_scorer(r2_score),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'model': pipeline,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_data': (X_test, y_test)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_feature_importance(self, segment_name):\n",
    "        \"\"\"Analyzes feature importance for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            return None\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        model = results['model']\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get numeric feature names\n",
    "        num_features = model.named_steps['preprocessor'].transformers_[0][2]\n",
    "        feature_names.extend(num_features)\n",
    "        \n",
    "        # Get encoded categorical feature names\n",
    "        cat_features = model.named_steps['preprocessor'].transformers_[1][2]\n",
    "        if len(cat_features) > 0:\n",
    "            encoder = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder']\n",
    "            if hasattr(encoder, 'get_feature_names_out'):\n",
    "                encoded_features = encoder.get_feature_names_out(cat_features)\n",
    "                feature_names.extend(encoded_features)\n",
    "        \n",
    "        # Get feature importances\n",
    "        if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "            importances = model.named_steps['regressor'].feature_importances_\n",
    "        else:\n",
    "            # For stacking regressor, use the average of base estimators\n",
    "            importances = np.mean([\n",
    "                est.feature_importances_ \n",
    "                for name, est in model.named_steps['regressor'].estimators_\n",
    "                if hasattr(est, 'feature_importances_')\n",
    "            ], axis=0)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    def train_all_segments(self):\n",
    "        \"\"\"Trains models for all segments and generates performance summary\"\"\"\n",
    "        # Define feature and target columns\n",
    "        feature_cols = [col for col in self.df.columns if col != 'Premium Amount']\n",
    "        target_col = 'Premium Amount'\n",
    "        \n",
    "        # Use the original dataframe directly\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # Train models for each segment\n",
    "        for name, mask in self.segments.items():\n",
    "            print(f\"\\nProcessing {name} segment...\")\n",
    "            X_seg = self.processed_df[feature_cols][mask]\n",
    "            y_seg = self.processed_df[target_col][mask]\n",
    "            print(f\"\\nSegment Length {len(X_seg)}...\")\n",
    "\n",
    "            if len(X_seg) >= 100:\n",
    "                results = self.train_segment_model(X_seg, y_seg, name)\n",
    "                self.segment_results[name] = results\n",
    "                \n",
    "                print(f\"Train R2: {results['train_metrics']['r2_score']:.4f}\")\n",
    "                print(f\"Test R2: {results['test_metrics']['r2_score']:.4f}\")\n",
    "                print(\"\\nPrediction Accuracy:\")\n",
    "                print(f\"Mean Absolute Error: {results['test_metrics']['mae']:.2f}\")\n",
    "                print(f\"Median Absolute Error: {results['test_metrics']['median_ae']:.2f}\")\n",
    "                print(f\"Mean % Error: {results['test_metrics']['mape']:.2f}%\")\n",
    "                print(f\"RMSE: {results['test_metrics']['rmse']:.2f}\")\n",
    "                print(f\"CV Mean R2: {results['cv_mean']:.4f} (+/- {results['cv_std']*2:.4f})\")\n",
    "        \n",
    "        # Create performance summary\n",
    "        performance_df = pd.DataFrame.from_dict(\n",
    "            {name: {\n",
    "                'segment_size': len(self.processed_df[mask]),\n",
    "                'train_r2': results['train_metrics']['r2_score'],\n",
    "                'test_r2': results['test_metrics']['r2_score'],\n",
    "                'cv_mean_r2': results['cv_mean'],\n",
    "                'cv_std_r2': results['cv_std'],\n",
    "                'mae': results['test_metrics']['mae'],\n",
    "                'mape': results['test_metrics']['mape']\n",
    "            } for name, results in self.segment_results.items()},\n",
    "            orient='index'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSegment Performance Summary:\")\n",
    "        print(performance_df.sort_values('test_r2', ascending=False))\n",
    "                \n",
    "        return performance_df\n",
    "    \n",
    "    def get_segment_predictions(self, segment_name, X_new):\n",
    "        \"\"\"Get predictions for new data using a trained segment model\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No trained model found for segment: {segment_name}\")\n",
    "        \n",
    "        model = self.segment_results[segment_name]['model']\n",
    "        return model.predict(X_new)\n",
    "    \n",
    "    def get_feature_importance(self, segment_name):\n",
    "        \"\"\"Get feature importance analysis for a specific segment\"\"\"\n",
    "        return self.analyze_feature_importance(segment_name)\n",
    "    \n",
    "    def get_segment_metrics(self, segment_name):\n",
    "        \"\"\"Get detailed performance metrics for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No results found for segment: {segment_name}\")\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        return {\n",
    "            'train_metrics': results['train_metrics'],\n",
    "            'test_metrics': results['test_metrics'],\n",
    "            'cv_scores': results['cv_scores'],\n",
    "            'cv_mean': results['cv_mean'],\n",
    "            'cv_std': results['cv_std']\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv('train.csv')\n",
    "    processed_df, _ = preprocess_insurance_data(train)\n",
    "    insurance_model = InsuranceSegmentModel(processed_df)\n",
    "    performance_summary = insurance_model.train_all_segments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NaN values before imputation:\n",
      "Age                      12489\n",
      "Gender                       0\n",
      "Annual Income            29860\n",
      "Marital Status           12336\n",
      "Number of Dependents     73130\n",
      "Education Level              0\n",
      "Occupation              239125\n",
      "Health Score             49449\n",
      "Location                     0\n",
      "Policy Type                  0\n",
      "Previous Claims         242802\n",
      "Vehicle Age                  3\n",
      "Credit Score             91451\n",
      "Insurance Duration           2\n",
      "Customer Feedback        52276\n",
      "Smoking Status               0\n",
      "Exercise Frequency           0\n",
      "Property Type                0\n",
      "dtype: int64\n",
      "\n",
      "NaN values after imputation:\n",
      "Age                     0\n",
      "Gender                  0\n",
      "Annual Income           0\n",
      "Marital Status          0\n",
      "Number of Dependents    0\n",
      "Education Level         0\n",
      "Occupation              0\n",
      "Health Score            0\n",
      "Location                0\n",
      "Policy Type             0\n",
      "Previous Claims         0\n",
      "Vehicle Age             0\n",
      "Credit Score            0\n",
      "Insurance Duration      0\n",
      "Customer Feedback       0\n",
      "Smoking Status          0\n",
      "Exercise Frequency      0\n",
      "Property Type           0\n",
      "dtype: int64\n",
      "\n",
      "Segment Distribution:\n",
      "Low_Risk_Premium: 171,016 (21.4%)\n",
      "High_Risk_Premium: 73,934 (9.2%)\n",
      "Young_Urban_Professional: 66,223 (8.3%)\n",
      "Family_Suburban: 143,868 (18.0%)\n",
      "Senior_Stable: 106,867 (13.4%)\n",
      "Budget_Basic: 106,350 (13.3%)\n",
      "Premium_Healthy: 48,518 (6.1%)\n",
      "High_Value_Property: 27,835 (3.5%)\n",
      "Default_Segment: 283,822 (35.5%)\n",
      "\n",
      "Total records: 800,000\n",
      "Total assigned: 1,028,433\n",
      "Records per segment on average: 114,270.3\n",
      "Predictions exported to predicted_premiums.csv.\n"
     ]
    }
   ],
   "source": [
    "def segment_test_data(df):\n",
    "\n",
    "    segments = segment_data(df)\n",
    "\n",
    "    # Apply the boolean mask to the DataFrame and return actual data segments\n",
    "    for key, mask in segments.items():\n",
    "        segments[key] = df[mask]\n",
    "\n",
    "    return segments\n",
    "\n",
    "def predict_and_export(test_df, model, output_file='predicted_premiums.csv'):\n",
    "    test_df_processed, test_ids = preprocess_insurance_data(test_df)\n",
    "    predictions = []\n",
    "\n",
    "    # Generate segments for the test data using the standalone function\n",
    "    segments = segment_test_data(test_df_processed)\n",
    "\n",
    "    for segment_name, test_segment in segments.items():\n",
    "        # Ensure there are indices in this segment\n",
    "        if not test_segment.empty:\n",
    "            test_segment_ids = test_ids[test_segment.index]\n",
    "\n",
    "            # Predict the segment\n",
    "            try:\n",
    "                predicted_values = model.get_segment_predictions(segment_name, test_segment)\n",
    "                # Collect ID and corresponding predictions\n",
    "                predictions.extend(zip(test_segment_ids, predicted_values))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing segment {segment_name}: {e}\")\n",
    "\n",
    "    # Convert predictions to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['id', 'Premium Amount'])\n",
    "\n",
    "    # Average the premium amounts for IDs with multiple entries\n",
    "    predictions_df = predictions_df.groupby('id')['Premium Amount'].mean().reset_index()\n",
    "\n",
    "    # Save the averaged results to CSV\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions exported to {output_file}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the test dataset\n",
    "    test = pd.read_csv('test.csv')\n",
    "\n",
    "    # Assuming the model is already instantiated and available as `insurance_model`\n",
    "    predict_and_export(test, insurance_model, 'predicted_premiums.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
