{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "def preprocess_insurance_data(df):\n",
    "    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Isolate the id column for return\n",
    "    ids = df['id'].copy()\n",
    "    \n",
    "    # Drop the 'id' column as it is just an identifier\n",
    "    df.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    # Replace infinite values with NaN globally\n",
    "    df.replace([float('inf'), float('-inf')], np.nan, inplace=True)\n",
    "    \n",
    "    # Convert Policy Start Date to datetime\n",
    "    df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'])\n",
    "    \n",
    "    # Use today's date dynamically\n",
    "    current_date = pd.Timestamp.now().normalize()  # normalize() sets time to midnight\n",
    "    \n",
    "    # Calculate days active\n",
    "    df['Days_Active'] = (current_date - df['Policy Start Date']).dt.days\n",
    "    \n",
    "    # Drop the Policy Start Date\n",
    "    df.drop('Policy Start Date', axis=1, inplace=True)\n",
    "    \n",
    "    # Print NaN status before imputation\n",
    "    print(\"\\nNaN values before imputation:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # Handle missing values through imputation\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # For numeric columns: use median imputation\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns: use mode imputation\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Print NaN status after imputation\n",
    "    print(\"\\nNaN values after imputation:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    return df, ids\n",
    "\n",
    "def segment_data(df):\n",
    "    segments = {\n",
    "        'Low_Risk_Premium': (\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.5)) &\n",
    "            (df['Health Score'] > df['Health Score'].quantile(0.5)) &\n",
    "            (df['Insurance Duration'] > 1)\n",
    "        ),\n",
    "        'High_Risk_Premium': (\n",
    "            (df['Previous Claims'] >= 2) &\n",
    "            ((df['Credit Score'] < df['Credit Score'].quantile(0.3)) |\n",
    "             (df['Health Score'] < df['Health Score'].quantile(0.3)))\n",
    "        ),\n",
    "        'Young_Urban_Professional': (\n",
    "            (df['Age'] <= df['Age'].quantile(0.4)) &\n",
    "            (df['Location'] == 'Urban') &\n",
    "            (df['Annual Income'] > df['Annual Income'].quantile(0.4)) &\n",
    "            (df['Insurance Duration'] > 0)\n",
    "        ),\n",
    "        'Family_Suburban': (\n",
    "            (df['Number of Dependents'] >= 1) &\n",
    "            (df['Location'].isin(['Suburban', 'Rural'])) &\n",
    "            (df['Marital Status'] == 'Married') &\n",
    "            (df['Insurance Duration'] > 0)\n",
    "        ),\n",
    "        'Senior_Stable': (\n",
    "            (df['Age'] >= df['Age'].quantile(0.6)) &\n",
    "            (df['Insurance Duration'] > df['Insurance Duration'].quantile(0.4)) &\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.4))\n",
    "        ),\n",
    "        'Budget_Basic': (\n",
    "            (df['Annual Income'] <= df['Annual Income'].quantile(0.4)) &\n",
    "            ((df['Policy Type'] == 'Basic') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Insurance Duration'] >= 0)\n",
    "        ),\n",
    "        'Premium_Healthy': (\n",
    "            ((df['Policy Type'] == 'Premium') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Exercise Frequency'].isin(['Daily', 'Weekly', 'Monthly'])) &\n",
    "            (df['Health Score'] > df['Health Score'].quantile(0.6)) &\n",
    "            (df['Annual Income'] > df['Annual Income'].quantile(0.4))\n",
    "        ),\n",
    "        'High_Value_Property': (\n",
    "            (df['Property Type'].isin(['House', 'Condo'])) &\n",
    "            (df['Annual Income'] >= df['Annual Income'].quantile(0.6)) &\n",
    "            ((df['Policy Type'] == 'Premium') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.5))\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Create a mask for all data assigned to a segment\n",
    "    assigned_mask = np.zeros(len(df), dtype=bool)\n",
    "    for mask in segments.values():\n",
    "        assigned_mask |= mask\n",
    "\n",
    "    # Create a default segment for unassigned data\n",
    "    segments['Default_Segment'] = ~assigned_mask\n",
    "\n",
    "    # Print distribution for debugging\n",
    "    total_records = len(df)\n",
    "    print(\"\\nSegment Distribution:\")\n",
    "    total_assigned = 0\n",
    "    for name, mask in segments.items():\n",
    "        segment_size = mask.sum()\n",
    "        total_assigned += segment_size\n",
    "        percentage = (segment_size / total_records) * 100\n",
    "        print(f\"{name}: {segment_size:,} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Additional debug info\n",
    "    print(f\"\\nTotal records: {total_records:,}\")\n",
    "    print(f\"Total assigned: {total_assigned:,}\")\n",
    "    print(f\"Records per segment on average: {total_assigned/len(segments):,.1f}\")\n",
    "\n",
    "    return segments\n",
    "\n",
    "class InsuranceSegmentModel:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize the model with a DataFrame\"\"\"\n",
    "        self.df = df\n",
    "        self.processed_df = None\n",
    "        self.segment_results = {}\n",
    "        self.cv_results = {}\n",
    "\n",
    "        self.segments = segment_data(df)\n",
    "\n",
    "        # Define model configurations\n",
    "        self.segment_configs = {\n",
    "            'High_Value_Property': {\n",
    "                'model': StackingRegressor(\n",
    "                    estimators=[\n",
    "                        ('rf', RandomForestRegressor(\n",
    "                            n_estimators=80,  # Reduced complexity\n",
    "                            max_depth=6,       # Lower depth to prevent overfitting\n",
    "                            min_samples_leaf=20,\n",
    "                            random_state=42\n",
    "                        )),\n",
    "                        ('gbm', GradientBoostingRegressor(\n",
    "                            n_estimators=80,  # Reduced complexity\n",
    "                            learning_rate=0.08,  # Slightly lower to improve generalization\n",
    "                            max_depth=4,  # Lower depth to prevent overfitting\n",
    "                            random_state=42\n",
    "                        ))\n",
    "                    ],\n",
    "                    final_estimator=LassoCV(cv=5, random_state=42),  # Increased CV folds\n",
    "                    cv=5,  # Increased CV folds\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "            },\n",
    "            'Budget_Basic': {\n",
    "                'model': RandomForestRegressor(  # Adjusted parameters for possibly better generalization\n",
    "                    n_estimators=200,\n",
    "                    max_depth=10,  # Slightly more depth to capture more complex patterns\n",
    "                    min_samples_leaf=25,  # Allowing more fine-grained leaf nodes\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                ),\n",
    "            },\n",
    "            'High_Risk_Premium': {\n",
    "                'model': GradientBoostingRegressor(  # Adjust parameters to improve fitting\n",
    "                    n_estimators=120,  # More estimators for better learning\n",
    "                    learning_rate=0.08,  # Slightly lower to improve stability\n",
    "                    max_depth=5,  # Increased depth to capture more complex relationships\n",
    "                    random_state=42\n",
    "                ),\n",
    "            },\n",
    "            'default': {\n",
    "                'model': RandomForestRegressor(\n",
    "                    n_estimators=120,  # Slightly more estimators\n",
    "                    max_depth=10,  # Increased depth to potentially improve model capture\n",
    "                    min_samples_leaf=25,  # Smaller leaf size for better granularity\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                ),\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_segment_pipeline(self, segment_name):\n",
    "        \"\"\"Creates a pipeline specific to a segment\"\"\"\n",
    "        config = self.segment_configs.get(segment_name, self.segment_configs['default'])\n",
    "        \n",
    "        # Define feature groups based on available columns\n",
    "        numeric_features = [col for col in self.df.select_dtypes(include=['int64', 'float64']).columns \n",
    "                        if col != 'Premium Amount']\n",
    "        \n",
    "        categorical_features = [col for col in self.df.select_dtypes(include=['object', 'category']).columns\n",
    "                            if col != 'Premium Amount']\n",
    "        \n",
    "        print(f\"\\nFeatures for {segment_name}:\")\n",
    "        print(\"Numeric features:\", numeric_features)\n",
    "        print(\"Categorical features:\", categorical_features)\n",
    "        \n",
    "        # Create transformers\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        \n",
    "        # Create preprocessor\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ],\n",
    "            sparse_threshold=0\n",
    "        )\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', config['model'])\n",
    "        ])\n",
    "        \n",
    "        # Print transformed feature names\n",
    "        pipeline.fit(self.df.head(1), [0])  # Fit on one row to get feature names\n",
    "        print(\"\\nTransformed feature names:\", pipeline.get_feature_names_out())\n",
    "        print(\"Total features:\", len(pipeline.get_feature_names_out()))\n",
    "        \n",
    "        return pipeline\n",
    "    \n",
    "    def evaluate_predictions(self, y_true, y_pred):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'r2_score': r2_score(y_true, y_pred),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
    "            'median_ae': np.median(np.abs(y_true - y_pred))\n",
    "        }\n",
    "    \n",
    "    def train_segment_model(self, X_seg, y_seg, segment_name):\n",
    "        \"\"\"Trains and evaluates a model for a specific customer segment\"\"\"\n",
    "        # Create pipeline\n",
    "        pipeline = self.create_segment_pipeline(segment_name)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_seg, y_seg, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = pipeline.predict(X_train)\n",
    "        test_predictions = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_metrics = self.evaluate_predictions(y_train, train_predictions)\n",
    "        test_metrics = self.evaluate_predictions(y_test, test_predictions)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, X_seg, y_seg,\n",
    "            cv=5,\n",
    "            scoring=make_scorer(r2_score),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'model': pipeline,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_data': (X_test, y_test)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_feature_importance(self, segment_name):\n",
    "        \"\"\"Analyzes feature importance for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            return None\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        model = results['model']\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get numeric feature names\n",
    "        num_features = model.named_steps['preprocessor'].transformers_[0][2]\n",
    "        feature_names.extend(num_features)\n",
    "        \n",
    "        # Get encoded categorical feature names\n",
    "        cat_features = model.named_steps['preprocessor'].transformers_[1][2]\n",
    "        if len(cat_features) > 0:\n",
    "            encoder = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder']\n",
    "            if hasattr(encoder, 'get_feature_names_out'):\n",
    "                encoded_features = encoder.get_feature_names_out(cat_features)\n",
    "                feature_names.extend(encoded_features)\n",
    "        \n",
    "        # Get feature importances\n",
    "        if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "            importances = model.named_steps['regressor'].feature_importances_\n",
    "        else:\n",
    "            # For stacking regressor, use the average of base estimators\n",
    "            importances = np.mean([\n",
    "                est.feature_importances_ \n",
    "                for name, est in model.named_steps['regressor'].estimators_\n",
    "                if hasattr(est, 'feature_importances_')\n",
    "            ], axis=0)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    def train_all_segments(self):\n",
    "        \"\"\"Trains models for all segments and generates performance summary\"\"\"\n",
    "        # Define feature and target columns\n",
    "        feature_cols = [col for col in self.df.columns if col != 'Premium Amount']\n",
    "        target_col = 'Premium Amount'\n",
    "        \n",
    "        # Use the original dataframe directly\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # Train models for each segment\n",
    "        for name, mask in self.segments.items():\n",
    "            print(f\"\\nProcessing {name} segment...\")\n",
    "            X_seg = self.processed_df[feature_cols][mask]\n",
    "            y_seg = self.processed_df[target_col][mask]\n",
    "            print(f\"\\nSegment Length {len(X_seg)}...\")\n",
    "\n",
    "            if len(X_seg) >= 100:\n",
    "                results = self.train_segment_model(X_seg, y_seg, name)\n",
    "                self.segment_results[name] = results\n",
    "                \n",
    "                print(f\"Train R2: {results['train_metrics']['r2_score']:.4f}\")\n",
    "                print(f\"Test R2: {results['test_metrics']['r2_score']:.4f}\")\n",
    "                print(\"\\nPrediction Accuracy:\")\n",
    "                print(f\"Mean Absolute Error: {results['test_metrics']['mae']:.2f}\")\n",
    "                print(f\"Median Absolute Error: {results['test_metrics']['median_ae']:.2f}\")\n",
    "                print(f\"Mean % Error: {results['test_metrics']['mape']:.2f}%\")\n",
    "                print(f\"RMSE: {results['test_metrics']['rmse']:.2f}\")\n",
    "                print(f\"CV Mean R2: {results['cv_mean']:.4f} (+/- {results['cv_std']*2:.4f})\")\n",
    "        \n",
    "        # Create performance summary\n",
    "        performance_df = pd.DataFrame.from_dict(\n",
    "            {name: {\n",
    "                'segment_size': len(self.processed_df[mask]),\n",
    "                'train_r2': results['train_metrics']['r2_score'],\n",
    "                'test_r2': results['test_metrics']['r2_score'],\n",
    "                'cv_mean_r2': results['cv_mean'],\n",
    "                'cv_std_r2': results['cv_std'],\n",
    "                'mae': results['test_metrics']['mae'],\n",
    "                'mape': results['test_metrics']['mape']\n",
    "            } for name, results in self.segment_results.items()},\n",
    "            orient='index'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSegment Performance Summary:\")\n",
    "        print(performance_df.sort_values('test_r2', ascending=False))\n",
    "                \n",
    "        return performance_df\n",
    "    \n",
    "    def get_segment_predictions(self, segment_name, X_new):\n",
    "        \"\"\"Get predictions for new data using a trained segment model\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No trained model found for segment: {segment_name}\")\n",
    "        \n",
    "        model = self.segment_results[segment_name]['model']\n",
    "        return model.predict(X_new)\n",
    "    \n",
    "    def get_feature_importance(self, segment_name):\n",
    "        \"\"\"Get feature importance analysis for a specific segment\"\"\"\n",
    "        return self.analyze_feature_importance(segment_name)\n",
    "    \n",
    "    def get_segment_metrics(self, segment_name):\n",
    "        \"\"\"Get detailed performance metrics for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No results found for segment: {segment_name}\")\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        return {\n",
    "            'train_metrics': results['train_metrics'],\n",
    "            'test_metrics': results['test_metrics'],\n",
    "            'cv_scores': results['cv_scores'],\n",
    "            'cv_mean': results['cv_mean'],\n",
    "            'cv_std': results['cv_std']\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv('train.csv')\n",
    "    processed_df, _ = preprocess_insurance_data(train)\n",
    "    insurance_model = InsuranceSegmentModel(processed_df)\n",
    "    performance_summary = insurance_model.train_all_segments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_test_data(df):\n",
    "\n",
    "    segments = segment_data(df)\n",
    "\n",
    "    # Apply the boolean mask to the DataFrame and return actual data segments\n",
    "    for key, mask in segments.items():\n",
    "        segments[key] = df[mask]\n",
    "\n",
    "    return segments\n",
    "\n",
    "def predict_and_export(test_df, model, output_file='predicted_premiums.csv'):\n",
    "    test_df_processed, test_ids = preprocess_insurance_data(test_df)\n",
    "    predictions = []\n",
    "\n",
    "    # Generate segments for the test data using the standalone function\n",
    "    segments = segment_test_data(test_df_processed)\n",
    "\n",
    "    for segment_name, test_segment in segments.items():\n",
    "        # Ensure there are indices in this segment\n",
    "        if not test_segment.empty:\n",
    "            print(f\"Segment {segment_name} processing with data:\")\n",
    "            print(test_segment.head())  # Check the first few rows to ensure data is present and correct\n",
    "            test_segment_ids = test_ids[test_segment.index]\n",
    "\n",
    "            # Predict the segment\n",
    "            try:\n",
    "                predicted_values = model.get_segment_predictions(segment_name, test_segment)\n",
    "                # Collect ID and corresponding predictions\n",
    "                predictions.extend(zip(test_segment_ids, predicted_values))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing segment {segment_name}: {e}\")\n",
    "\n",
    "    # Convert predictions to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['id', 'Premium Amount'])\n",
    "\n",
    "    # Average the premium amounts for IDs with multiple entries\n",
    "    predictions_df = predictions_df.groupby('id')['Premium Amount'].mean().reset_index()\n",
    "\n",
    "    # Save the averaged results to CSV\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions exported to {output_file}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the test dataset\n",
    "    test = pd.read_csv('test.csv')\n",
    "    # Assuming the model is already instantiated and available as `insurance_model`\n",
    "    predict_and_export(test, insurance_model, 'predicted_premiums.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
