{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score, mean_absolute_error, mean_squared_error, make_scorer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNNImputer\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_insurance_data\u001b[39m(df):\n\u001b[1;32m     15\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.impute import KNNImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def preprocess_insurance_data(df):\n",
    "    df = df.copy()\n",
    "    ids = df['id'].copy()\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df.drop(['id', 'Policy Start Date'], axis=1, inplace=True)\n",
    "    df.replace([float('inf'), float('-inf')], np.nan, inplace=True)\n",
    "    \n",
    "    print(\"\\nNaN values before imputation:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # First handle categorical variables using complete columns\n",
    "    # Group by complete columns that might have logical relationships\n",
    "    \n",
    "    # Occupation (358,075 missing)\n",
    "    # Education Level and Location are complete and likely related to occupation\n",
    "    df['Occupation'] = df.groupby(['Education Level', 'Location'], observed=True)['Occupation'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "    )\n",
    "    \n",
    "    # Health Score (74,076 missing) could relate to Exercise Frequency and Smoking Status\n",
    "    df['Health Score'] = df.groupby(['Exercise Frequency', 'Smoking Status'], observed=True)['Health Score'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    \n",
    "    # Credit Score (137,882 missing) might relate to Property Type and Policy Type\n",
    "    df['Credit Score'] = df.groupby(['Property Type', 'Policy Type'], observed=True)['Credit Score'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    \n",
    "    # Marital Status (18,529 missing) might relate to Property Type\n",
    "    df['Marital Status'] = df.groupby('Property Type', observed=True)['Marital Status'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "    )\n",
    "    \n",
    "    # For remaining numeric columns, use grouping by Policy Type and Property Type\n",
    "    numeric_cols = ['Age', 'Annual Income', 'Number of Dependents', 'Previous Claims', \n",
    "                   'Vehicle Age', 'Insurance Duration']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df[col] = df.groupby(['Policy Type', 'Property Type'], observed=True)[col].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "    \n",
    "    # Customer Feedback can use Policy Type\n",
    "    df['Customer Feedback'] = df.groupby('Policy Type', observed=True)['Customer Feedback'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nNaN values after imputation:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    return df, ids\n",
    "\n",
    "def segment_data(df):\n",
    "    segments = {\n",
    "        'Low_Risk_Premium': (\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.5)) &\n",
    "            (df['Health Score'] > df['Health Score'].quantile(0.5)) &\n",
    "            (df['Insurance Duration'] > 1)\n",
    "        ),\n",
    "        'High_Risk_Premium': (\n",
    "            (df['Previous Claims'] >= 2) &\n",
    "            ((df['Credit Score'] < df['Credit Score'].quantile(0.3)) |\n",
    "             (df['Health Score'] < df['Health Score'].quantile(0.3)))\n",
    "        ),\n",
    "        'Young_Urban_Professional': (\n",
    "            (df['Age'] <= df['Age'].quantile(0.4)) &\n",
    "            (df['Location'] == 'Urban') &\n",
    "            (df['Annual Income'] > df['Annual Income'].quantile(0.4)) &\n",
    "            (df['Insurance Duration'] > 0)\n",
    "        ),\n",
    "        'Family_Suburban': (\n",
    "            (df['Number of Dependents'] >= 1) &\n",
    "            (df['Location'].isin(['Suburban', 'Rural'])) &\n",
    "            (df['Marital Status'] == 'Married') &\n",
    "            (df['Insurance Duration'] > 0)\n",
    "        ),\n",
    "        'Senior_Stable': (\n",
    "            (df['Age'] >= df['Age'].quantile(0.6)) &\n",
    "            (df['Insurance Duration'] > df['Insurance Duration'].quantile(0.4)) &\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.4))\n",
    "        ),\n",
    "        'Budget_Basic': (\n",
    "            (df['Annual Income'] <= df['Annual Income'].quantile(0.4)) &\n",
    "            ((df['Policy Type'] == 'Basic') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Insurance Duration'] >= 0)\n",
    "        ),\n",
    "        'Premium_Healthy': (\n",
    "            ((df['Policy Type'] == 'Premium') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Exercise Frequency'].isin(['Daily', 'Weekly', 'Monthly'])) &\n",
    "            (df['Health Score'] > df['Health Score'].quantile(0.6)) &\n",
    "            (df['Annual Income'] > df['Annual Income'].quantile(0.4))\n",
    "        ),\n",
    "        'High_Value_Property': (\n",
    "            (df['Property Type'].isin(['House', 'Condo'])) &\n",
    "            (df['Annual Income'] >= df['Annual Income'].quantile(0.6)) &\n",
    "            ((df['Policy Type'] == 'Premium') | \n",
    "             (df['Policy Type'] == 'Standard')) &\n",
    "            (df['Credit Score'] > df['Credit Score'].quantile(0.5))\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Create a mask for all data assigned to a segment\n",
    "    assigned_mask = np.zeros(len(df), dtype=bool)\n",
    "    for mask in segments.values():\n",
    "        assigned_mask |= mask\n",
    "\n",
    "    # Create a default segment for unassigned data\n",
    "    segments['Default_Segment'] = ~assigned_mask\n",
    "\n",
    "    # Print distribution for debugging\n",
    "    total_records = len(df)\n",
    "    print(\"\\nSegment Distribution:\")\n",
    "    total_assigned = 0\n",
    "    for name, mask in segments.items():\n",
    "        segment_size = mask.sum()\n",
    "        total_assigned += segment_size\n",
    "        percentage = (segment_size / total_records) * 100\n",
    "        print(f\"{name}: {segment_size:,} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Additional debug info\n",
    "    print(f\"\\nTotal records: {total_records:,}\")\n",
    "    print(f\"Total assigned: {total_assigned:,}\")\n",
    "    print(f\"Records per segment on average: {total_assigned/len(segments):,.1f}\")\n",
    "\n",
    "    return segments\n",
    "\n",
    "class InsuranceSegmentModel:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize the model with a DataFrame\"\"\"\n",
    "        self.df = df\n",
    "        self.processed_df = None\n",
    "        self.segment_results = {}\n",
    "        self.cv_results = {}\n",
    "\n",
    "        # NEW: Create standard category mappings at initialization\n",
    "        self.categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if 'Premium Amount' in self.categorical_features:\n",
    "            self.categorical_features.remove('Premium Amount')\n",
    "            \n",
    "        self.category_mappings = {\n",
    "            col: sorted(df[col].unique()) for col in self.categorical_features\n",
    "        }\n",
    "\n",
    "        self.segments = segment_data(df)\n",
    "\n",
    "        self.segment_configs = {\n",
    "            'default': {\n",
    "                'model': XGBRegressor(\n",
    "                    n_estimators=100,\n",
    "                    learning_rate=0.01,\n",
    "                    max_depth=3,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42\n",
    "                ),\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def create_segment_pipeline(self, segment_name):\n",
    "        \"\"\"Creates a pipeline specific to a segment\"\"\"\n",
    "        config = self.segment_configs.get(segment_name, self.segment_configs['default'])\n",
    "        \n",
    "        # Define feature groups\n",
    "        numeric_features = self.df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        if 'Premium Amount' in numeric_features:\n",
    "            numeric_features.remove('Premium Amount')\n",
    "                \n",
    "        # Create transformers\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # CHANGED: Use predefined category mappings\n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(\n",
    "                categories=[self.category_mappings[col] for col in self.categorical_features],\n",
    "                sparse_output=False,\n",
    "                handle_unknown='ignore'\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Create preprocessor\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, self.categorical_features)\n",
    "            ],\n",
    "            sparse_threshold=0\n",
    "        )\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', config['model'])\n",
    "        ])\n",
    "        \n",
    "        return pipeline\n",
    "        \n",
    "    def evaluate_predictions(self, y_true, y_pred):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'r2_score': r2_score(y_true, y_pred),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
    "            'median_ae': np.median(np.abs(y_true - y_pred))\n",
    "        }\n",
    "    \n",
    "    def train_segment_model(self, X_seg, y_seg, segment_name):\n",
    "        \"\"\"Trains and evaluates a model for a specific customer segment\"\"\"\n",
    "        # Create pipeline\n",
    "        pipeline = self.create_segment_pipeline(segment_name)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_seg, y_seg, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = pipeline.predict(X_train)\n",
    "        test_predictions = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_metrics = self.evaluate_predictions(y_train, train_predictions)\n",
    "        test_metrics = self.evaluate_predictions(y_test, test_predictions)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, X_seg, y_seg,\n",
    "            cv=5,\n",
    "            scoring=make_scorer(r2_score),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'model': pipeline,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_data': (X_test, y_test)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_feature_importance(self, segment_name):\n",
    "        \"\"\"Analyzes feature importance for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            return None\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        model = results['model']\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get numeric feature names\n",
    "        num_features = model.named_steps['preprocessor'].transformers_[0][2]\n",
    "        feature_names.extend(num_features)\n",
    "        \n",
    "        # Get encoded categorical feature names\n",
    "        cat_features = model.named_steps['preprocessor'].transformers_[1][2]\n",
    "        if len(cat_features) > 0:\n",
    "            encoder = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder']\n",
    "            if hasattr(encoder, 'get_feature_names_out'):\n",
    "                encoded_features = encoder.get_feature_names_out(cat_features)\n",
    "                feature_names.extend(encoded_features)\n",
    "        \n",
    "        # Get feature importances\n",
    "        if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "            importances = model.named_steps['regressor'].feature_importances_\n",
    "        else:\n",
    "            # For stacking regressor, use the average of base estimators\n",
    "            importances = np.mean([\n",
    "                est.feature_importances_ \n",
    "                for name, est in model.named_steps['regressor'].estimators_\n",
    "                if hasattr(est, 'feature_importances_')\n",
    "            ], axis=0)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    def train_all_segments(self):\n",
    "        \"\"\"Trains models for all segments and generates performance summary\"\"\"\n",
    "        # Define feature and target columns\n",
    "        feature_cols = [col for col in self.df.columns if col != 'Premium Amount']\n",
    "        target_col = 'Premium Amount'\n",
    "        \n",
    "        # Use the original dataframe directly\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # Train models for each segment\n",
    "        for name, mask in self.segments.items():\n",
    "            print(f\"\\nProcessing {name} segment...\")\n",
    "            X_seg = self.processed_df[feature_cols][mask]\n",
    "            y_seg = self.processed_df[target_col][mask]\n",
    "            print(f\"\\nSegment Length {len(X_seg)}...\")\n",
    "\n",
    "            if len(X_seg) >= 100:\n",
    "                results = self.train_segment_model(X_seg, y_seg, name)\n",
    "                self.segment_results[name] = results\n",
    "                \n",
    "                print(f\"Train R2: {results['train_metrics']['r2_score']:.4f}\")\n",
    "                print(f\"Test R2: {results['test_metrics']['r2_score']:.4f}\")\n",
    "                print(\"\\nPrediction Accuracy:\")\n",
    "                print(f\"Mean Absolute Error: {results['test_metrics']['mae']:.2f}\")\n",
    "                print(f\"Median Absolute Error: {results['test_metrics']['median_ae']:.2f}\")\n",
    "                print(f\"Mean % Error: {results['test_metrics']['mape']:.2f}%\")\n",
    "                print(f\"RMSE: {results['test_metrics']['rmse']:.2f}\")\n",
    "                print(f\"CV Mean R2: {results['cv_mean']:.4f} (+/- {results['cv_std']*2:.4f})\")\n",
    "        \n",
    "        # Create performance summary\n",
    "        performance_df = pd.DataFrame.from_dict(\n",
    "            {name: {\n",
    "                'segment_size': len(self.processed_df[mask]),\n",
    "                'train_r2': results['train_metrics']['r2_score'],\n",
    "                'test_r2': results['test_metrics']['r2_score'],\n",
    "                'cv_mean_r2': results['cv_mean'],\n",
    "                'cv_std_r2': results['cv_std'],\n",
    "                'mae': results['test_metrics']['mae'],\n",
    "                'mape': results['test_metrics']['mape']\n",
    "            } for name, results in self.segment_results.items()},\n",
    "            orient='index'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSegment Performance Summary:\")\n",
    "        print(performance_df.sort_values('test_r2', ascending=False))\n",
    "                \n",
    "        return performance_df\n",
    "    \n",
    "    def get_segment_predictions(self, segment_name, X_new):\n",
    "        \"\"\"Get predictions for new data using a trained segment model\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No trained model found for segment: {segment_name}\")\n",
    "        \n",
    "        model = self.segment_results[segment_name]['model']\n",
    "        return model.predict(X_new)\n",
    "    \n",
    "    def get_feature_importance(self, segment_name):\n",
    "        \"\"\"Get feature importance analysis for a specific segment\"\"\"\n",
    "        return self.analyze_feature_importance(segment_name)\n",
    "    \n",
    "    def get_segment_metrics(self, segment_name):\n",
    "        \"\"\"Get detailed performance metrics for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No results found for segment: {segment_name}\")\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        return {\n",
    "            'train_metrics': results['train_metrics'],\n",
    "            'test_metrics': results['test_metrics'],\n",
    "            'cv_scores': results['cv_scores'],\n",
    "            'cv_mean': results['cv_mean'],\n",
    "            'cv_std': results['cv_std']\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv('train.csv')\n",
    "    processed_df, _ = preprocess_insurance_data(train)\n",
    "    insurance_model = InsuranceSegmentModel(processed_df)\n",
    "    performance_summary = insurance_model.train_all_segments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_test_data(df):\n",
    "\n",
    "    segments = segment_data(df)\n",
    "\n",
    "    # Apply the boolean mask to the DataFrame and return actual data segments\n",
    "    for key, mask in segments.items():\n",
    "        segments[key] = df[mask]\n",
    "\n",
    "    return segments\n",
    "\n",
    "def predict_and_export(test_df, model, output_file='predicted_premiums.csv'):\n",
    "    test_df_processed, test_ids = preprocess_insurance_data(test_df)\n",
    "    predictions = []\n",
    "\n",
    "    # Generate segments for the test data using the standalone function\n",
    "    segments = segment_test_data(test_df_processed)\n",
    "\n",
    "    for segment_name, test_segment in segments.items():\n",
    "        # Ensure there are indices in this segment\n",
    "        if not test_segment.empty:\n",
    "            test_segment_ids = test_ids[test_segment.index]\n",
    "\n",
    "            # Predict the segment\n",
    "            try:\n",
    "                predicted_values = model.get_segment_predictions(segment_name, test_segment)\n",
    "                # Collect ID and corresponding predictions\n",
    "                predictions.extend(zip(test_segment_ids, predicted_values))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing segment {segment_name}: {e}\")\n",
    "\n",
    "    # Convert predictions to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['id', 'Premium Amount'])\n",
    "\n",
    "    # Average the premium amounts for IDs with multiple entries\n",
    "    predictions_df = predictions_df.groupby('id')['Premium Amount'].mean().reset_index()\n",
    "\n",
    "    # Save the averaged results to CSV\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions exported to {output_file}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the test dataset\n",
    "    test = pd.read_csv('test.csv')\n",
    "\n",
    "    # Assuming the model is already instantiated and available as `insurance_model`\n",
    "    predict_and_export(test, insurance_model, 'predicted_premiums.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
