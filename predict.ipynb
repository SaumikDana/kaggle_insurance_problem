{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Low_Risk_Premium segment...\n",
      "\n",
      "Segment Length 27712...\n",
      "Train R2: 0.1660\n",
      "Test R2: 0.0655\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 643.83\n",
      "Median Absolute Error: 482.23\n",
      "Mean % Error: 344.41%\n",
      "RMSE: 864.84\n",
      "CV Mean R2: 0.0644 (+/- 0.0178)\n",
      "\n",
      "Processing High_Risk_Premium segment...\n",
      "\n",
      "Segment Length 209187...\n",
      "Train R2: 0.1266\n",
      "Test R2: 0.0315\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 650.12\n",
      "Median Absolute Error: 517.11\n",
      "Mean % Error: 303.21%\n",
      "RMSE: 852.93\n",
      "CV Mean R2: 0.0344 (+/- 0.0051)\n",
      "\n",
      "Processing Young_Urban_Professional segment...\n",
      "\n",
      "Segment Length 17449...\n",
      "Train R2: 0.2062\n",
      "Test R2: 0.0721\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 647.99\n",
      "Median Absolute Error: 495.64\n",
      "Mean % Error: 309.15%\n",
      "RMSE: 866.65\n",
      "CV Mean R2: 0.0771 (+/- 0.0137)\n",
      "\n",
      "Processing Family_Suburban segment...\n",
      "\n",
      "Segment Length 11696...\n",
      "Train R2: 0.1736\n",
      "Test R2: 0.0391\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 633.46\n",
      "Median Absolute Error: 494.60\n",
      "Mean % Error: 307.60%\n",
      "RMSE: 839.35\n",
      "CV Mean R2: 0.0354 (+/- 0.0357)\n",
      "\n",
      "Processing Senior_Stable segment...\n",
      "\n",
      "Segment Length 19689...\n",
      "Train R2: 0.1854\n",
      "Test R2: 0.0670\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 637.33\n",
      "Median Absolute Error: 487.55\n",
      "Mean % Error: 319.42%\n",
      "RMSE: 851.92\n",
      "CV Mean R2: 0.0726 (+/- 0.0279)\n",
      "\n",
      "Processing Budget_Basic segment...\n",
      "\n",
      "Segment Length 33869...\n",
      "Train R2: 0.1134\n",
      "Test R2: 0.0261\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 661.89\n",
      "Median Absolute Error: 521.48\n",
      "Mean % Error: 343.20%\n",
      "RMSE: 857.34\n",
      "CV Mean R2: 0.0262 (+/- 0.0056)\n",
      "\n",
      "Processing Premium_Healthy segment...\n",
      "\n",
      "Segment Length 9628...\n",
      "Train R2: 0.2170\n",
      "Test R2: 0.0619\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 651.73\n",
      "Median Absolute Error: 503.90\n",
      "Mean % Error: 318.59%\n",
      "RMSE: 859.61\n",
      "CV Mean R2: 0.0707 (+/- 0.0145)\n",
      "\n",
      "Processing High_Value_Property segment...\n",
      "\n",
      "Segment Length 2805...\n",
      "Train R2: 0.3003\n",
      "Test R2: 0.1537\n",
      "\n",
      "Prediction Accuracy:\n",
      "Mean Absolute Error: 787.45\n",
      "Median Absolute Error: 567.98\n",
      "Mean % Error: 564.10%\n",
      "RMSE: 1065.96\n",
      "CV Mean R2: 0.1871 (+/- 0.0704)\n",
      "\n",
      "Segment Performance Summary:\n",
      "                          segment_size  train_r2   test_r2  cv_mean_r2  \\\n",
      "High_Value_Property               2805  0.300289  0.153742    0.187058   \n",
      "Young_Urban_Professional          2805  0.206234  0.072077    0.077061   \n",
      "Senior_Stable                     2805  0.185398  0.066961    0.072596   \n",
      "Low_Risk_Premium                  2805  0.166013  0.065456    0.064425   \n",
      "Premium_Healthy                   2805  0.217024  0.061938    0.070724   \n",
      "Family_Suburban                   2805  0.173638  0.039116    0.035374   \n",
      "High_Risk_Premium                 2805  0.126613  0.031511    0.034407   \n",
      "Budget_Basic                      2805  0.113401  0.026110    0.026156   \n",
      "\n",
      "                          cv_std_r2         mae        mape  \n",
      "High_Value_Property        0.035201  787.453440  564.095402  \n",
      "Young_Urban_Professional   0.006853  647.992043  309.146837  \n",
      "Senior_Stable              0.013945  637.333731  319.416860  \n",
      "Low_Risk_Premium           0.008903  643.830812  344.413465  \n",
      "Premium_Healthy            0.007273  651.732432  318.586457  \n",
      "Family_Suburban            0.017854  633.461313  307.597232  \n",
      "High_Risk_Premium          0.002569  650.121077  303.212477  \n",
      "Budget_Basic               0.002794  661.892891  343.201039  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "def preprocess_insurance_data(df):\n",
    "\n",
    "    # Drop the 'id' column as it is just an identifier\n",
    "    df.drop('id', axis=1, inplace=True)\n",
    "\n",
    "    # Replace infinite values with NaN globally\n",
    "    df.replace([float('inf'), float('-inf')], np.nan, inplace=True)\n",
    "\n",
    "    # Convert Policy Start Date to datetime\n",
    "    df['Policy Start Date'] = pd.to_datetime(train['Policy Start Date'])\n",
    "\n",
    "    # Use today's date dynamically\n",
    "    current_date = pd.Timestamp.now().normalize()  # normalize() sets time to midnight\n",
    "\n",
    "    # Calculate days active\n",
    "    df['Days_Active'] = (current_date - train['Policy Start Date']).dt.days\n",
    "\n",
    "    # Drop the Policy Start Date\n",
    "    df.drop('Policy Start Date', axis=1, inplace=True)\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "class InsuranceSegmentModel:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize the model with a DataFrame\"\"\"\n",
    "        self.df = df\n",
    "        self.processed_df = None\n",
    "        self.segment_results = {}\n",
    "        self.cv_results = {}\n",
    "\n",
    "        # Simplified segment definitions:\n",
    "        self.segments = {\n",
    "            'Low_Risk_Premium': (\n",
    "                (self.df['Credit Score'] > self.df['Credit Score'].quantile(0.6)) & \n",
    "                (self.df['Health Score'] > self.df['Health Score'].quantile(0.6)) & \n",
    "                (self.df['Insurance Duration'] > self.df['Insurance Duration'].median())\n",
    "            ),\n",
    "            'High_Risk_Premium': (\n",
    "                (self.df['Previous Claims'] >= 3) | \n",
    "                (self.df['Credit Score'] < self.df['Credit Score'].quantile(0.3)) |\n",
    "                (self.df['Health Score'] < self.df['Health Score'].quantile(0.3))\n",
    "            ),\n",
    "            'Young_Urban_Professional': (\n",
    "                (self.df['Age'] <= self.df['Age'].quantile(0.3)) & \n",
    "                (self.df['Location'] == 'Urban') & \n",
    "                (self.df['Annual Income'] > self.df['Annual Income'].median()) & \n",
    "                (self.df['Insurance Duration'] > 1)\n",
    "            ),\n",
    "            'Family_Suburban': (\n",
    "                (self.df['Number of Dependents'] >= 2) & \n",
    "                (self.df['Location'] == 'Suburban') & \n",
    "                (self.df['Marital Status'] == 'Married') & \n",
    "                (self.df['Insurance Duration'] > self.df['Insurance Duration'].median())\n",
    "            ),\n",
    "            'Senior_Stable': (\n",
    "                (self.df['Age'] >= self.df['Age'].quantile(0.7)) & \n",
    "                (self.df['Insurance Duration'] > self.df['Insurance Duration'].quantile(0.6)) & \n",
    "                (self.df['Credit Score'] > self.df['Credit Score'].median())\n",
    "            ),\n",
    "            'Budget_Basic': (\n",
    "                (self.df['Annual Income'] <= self.df['Annual Income'].quantile(0.3)) & \n",
    "                (self.df['Policy Type'] == 'Basic') & \n",
    "                (self.df['Insurance Duration'] > 1)\n",
    "            ),\n",
    "            'Premium_Healthy': (\n",
    "                (self.df['Policy Type'] == 'Premium') & \n",
    "                (self.df['Exercise Frequency'].isin(['Daily', 'Weekly'])) & \n",
    "                (self.df['Health Score'] > self.df['Health Score'].quantile(0.7)) & \n",
    "                (self.df['Annual Income'] > self.df['Annual Income'].median())\n",
    "            ),\n",
    "            'High_Value_Property': (\n",
    "                (self.df['Property Type'] == 'House') & \n",
    "                (self.df['Annual Income'] >= self.df['Annual Income'].quantile(0.75)) & \n",
    "                (self.df['Policy Type'] == 'Premium') & \n",
    "                (self.df['Credit Score'] > self.df['Credit Score'].quantile(0.6))\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Define model configurations\n",
    "        self.segment_configs = {\n",
    "            'High_Value_Property': {\n",
    "                'model': StackingRegressor(\n",
    "                    estimators=[\n",
    "                        ('rf', RandomForestRegressor(\n",
    "                            n_estimators=80,  # Reduced complexity\n",
    "                            max_depth=6,       # Lower depth to prevent overfitting\n",
    "                            min_samples_leaf=20,\n",
    "                            random_state=42\n",
    "                        )),\n",
    "                        ('gbm', GradientBoostingRegressor(\n",
    "                            n_estimators=80,  # Reduced complexity\n",
    "                            learning_rate=0.08,  # Slightly lower to improve generalization\n",
    "                            max_depth=4,  # Lower depth to prevent overfitting\n",
    "                            random_state=42\n",
    "                        ))\n",
    "                    ],\n",
    "                    final_estimator=LassoCV(cv=5, random_state=42),  # Increased CV folds\n",
    "                    cv=5,  # Increased CV folds\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "            },\n",
    "            'Budget_Basic': {\n",
    "                'model': RandomForestRegressor(  # Adjusted parameters for possibly better generalization\n",
    "                    n_estimators=200,\n",
    "                    max_depth=10,  # Slightly more depth to capture more complex patterns\n",
    "                    min_samples_leaf=25,  # Allowing more fine-grained leaf nodes\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                ),\n",
    "            },\n",
    "            'High_Risk_Premium': {\n",
    "                'model': GradientBoostingRegressor(  # Adjust parameters to improve fitting\n",
    "                    n_estimators=200,  # More estimators for better learning\n",
    "                    learning_rate=0.08,  # Slightly lower to improve stability\n",
    "                    max_depth=7,  # Increased depth to capture more complex relationships\n",
    "                    random_state=42\n",
    "                ),\n",
    "            },\n",
    "            'default': {\n",
    "                'model': RandomForestRegressor(\n",
    "                    n_estimators=120,  # Slightly more estimators\n",
    "                    max_depth=10,  # Increased depth to potentially improve model capture\n",
    "                    min_samples_leaf=25,  # Smaller leaf size for better granularity\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                ),\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_segment_pipeline(self, segment_name):\n",
    "        \"\"\"Creates a pipeline specific to a segment\"\"\"\n",
    "        config = self.segment_configs.get(segment_name, self.segment_configs['default'])\n",
    "        \n",
    "        # Define feature groups based on available columns\n",
    "        numeric_features = [col for col in self.df.select_dtypes(include=['int64', 'float64']).columns \n",
    "                          if col != 'Premium Amount']\n",
    "        \n",
    "        categorical_features = [col for col in self.df.select_dtypes(include=['object', 'category']).columns\n",
    "                              if col != 'Premium Amount']\n",
    "        \n",
    "        # Create transformers\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        \n",
    "        # Create preprocessor\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ],\n",
    "            sparse_threshold=0\n",
    "        )\n",
    "        \n",
    "        # Create pipeline\n",
    "        return Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', config['model'])\n",
    "        ])\n",
    "    \n",
    "    def evaluate_predictions(self, y_true, y_pred):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'r2_score': r2_score(y_true, y_pred),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
    "            'median_ae': np.median(np.abs(y_true - y_pred))\n",
    "        }\n",
    "    \n",
    "    def train_segment_model(self, X_seg, y_seg, segment_name):\n",
    "        \"\"\"Trains and evaluates a model for a specific customer segment\"\"\"\n",
    "        # Create pipeline\n",
    "        pipeline = self.create_segment_pipeline(segment_name)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_seg, y_seg, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = pipeline.predict(X_train)\n",
    "        test_predictions = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_metrics = self.evaluate_predictions(y_train, train_predictions)\n",
    "        test_metrics = self.evaluate_predictions(y_test, test_predictions)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, X_seg, y_seg,\n",
    "            cv=5,\n",
    "            scoring=make_scorer(r2_score),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'model': pipeline,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_data': (X_test, y_test)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_feature_importance(self, segment_name):\n",
    "        \"\"\"Analyzes feature importance for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            return None\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        model = results['model']\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get numeric feature names\n",
    "        num_features = model.named_steps['preprocessor'].transformers_[0][2]\n",
    "        feature_names.extend(num_features)\n",
    "        \n",
    "        # Get encoded categorical feature names\n",
    "        cat_features = model.named_steps['preprocessor'].transformers_[1][2]\n",
    "        if len(cat_features) > 0:\n",
    "            encoder = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder']\n",
    "            if hasattr(encoder, 'get_feature_names_out'):\n",
    "                encoded_features = encoder.get_feature_names_out(cat_features)\n",
    "                feature_names.extend(encoded_features)\n",
    "        \n",
    "        # Get feature importances\n",
    "        if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "            importances = model.named_steps['regressor'].feature_importances_\n",
    "        else:\n",
    "            # For stacking regressor, use the average of base estimators\n",
    "            importances = np.mean([\n",
    "                est.feature_importances_ \n",
    "                for name, est in model.named_steps['regressor'].estimators_\n",
    "                if hasattr(est, 'feature_importances_')\n",
    "            ], axis=0)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    def train_all_segments(self):\n",
    "        \"\"\"Trains models for all segments and generates performance summary\"\"\"\n",
    "        # Define feature and target columns\n",
    "        feature_cols = [col for col in self.df.columns if col != 'Premium Amount']\n",
    "        target_col = 'Premium Amount'\n",
    "        \n",
    "        # Use the original dataframe directly\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # Train models for each segment\n",
    "        for name, mask in self.segments.items():\n",
    "            print(f\"\\nProcessing {name} segment...\")\n",
    "            X_seg = self.processed_df[feature_cols][mask]\n",
    "            y_seg = self.processed_df[target_col][mask]\n",
    "            print(f\"\\nSegment Length {len(X_seg)}...\")\n",
    "\n",
    "            if len(X_seg) >= 100:\n",
    "                results = self.train_segment_model(X_seg, y_seg, name)\n",
    "                self.segment_results[name] = results\n",
    "                \n",
    "                print(f\"Train R2: {results['train_metrics']['r2_score']:.4f}\")\n",
    "                print(f\"Test R2: {results['test_metrics']['r2_score']:.4f}\")\n",
    "                print(\"\\nPrediction Accuracy:\")\n",
    "                print(f\"Mean Absolute Error: {results['test_metrics']['mae']:.2f}\")\n",
    "                print(f\"Median Absolute Error: {results['test_metrics']['median_ae']:.2f}\")\n",
    "                print(f\"Mean % Error: {results['test_metrics']['mape']:.2f}%\")\n",
    "                print(f\"RMSE: {results['test_metrics']['rmse']:.2f}\")\n",
    "                print(f\"CV Mean R2: {results['cv_mean']:.4f} (+/- {results['cv_std']*2:.4f})\")\n",
    "        \n",
    "        # Create performance summary\n",
    "        performance_df = pd.DataFrame.from_dict(\n",
    "            {name: {\n",
    "                'segment_size': len(self.processed_df[mask]),\n",
    "                'train_r2': results['train_metrics']['r2_score'],\n",
    "                'test_r2': results['test_metrics']['r2_score'],\n",
    "                'cv_mean_r2': results['cv_mean'],\n",
    "                'cv_std_r2': results['cv_std'],\n",
    "                'mae': results['test_metrics']['mae'],\n",
    "                'mape': results['test_metrics']['mape']\n",
    "            } for name, results in self.segment_results.items()},\n",
    "            orient='index'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSegment Performance Summary:\")\n",
    "        print(performance_df.sort_values('test_r2', ascending=False))\n",
    "                \n",
    "        return performance_df\n",
    "    \n",
    "    def get_segment_predictions(self, segment_name, X_new):\n",
    "        \"\"\"Get predictions for new data using a trained segment model\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No trained model found for segment: {segment_name}\")\n",
    "        \n",
    "        model = self.segment_results[segment_name]['model']\n",
    "        return model.predict(X_new)\n",
    "    \n",
    "    def get_feature_importance(self, segment_name):\n",
    "        \"\"\"Get feature importance analysis for a specific segment\"\"\"\n",
    "        return self.analyze_feature_importance(segment_name)\n",
    "    \n",
    "    def get_segment_metrics(self, segment_name):\n",
    "        \"\"\"Get detailed performance metrics for a specific segment\"\"\"\n",
    "        if segment_name not in self.segment_results:\n",
    "            raise ValueError(f\"No results found for segment: {segment_name}\")\n",
    "        \n",
    "        results = self.segment_results[segment_name]\n",
    "        return {\n",
    "            'train_metrics': results['train_metrics'],\n",
    "            'test_metrics': results['test_metrics'],\n",
    "            'cv_scores': results['cv_scores'],\n",
    "            'cv_mean': results['cv_mean'],\n",
    "            'cv_std': results['cv_std']\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv('train.csv')\n",
    "    processed_df = preprocess_insurance_data(train)\n",
    "    insurance_model = InsuranceSegmentModel(processed_df)\n",
    "    performance_summary = insurance_model.train_all_segments()\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
